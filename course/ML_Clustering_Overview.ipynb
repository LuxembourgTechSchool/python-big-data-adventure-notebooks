{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Technique 3 - Clustering\n",
    "\n",
    "# 1. Group Newsgroup Posts by Topic\n",
    "\n",
    "This notebook will demonstrate how to use basic scikit functionalities to cluster objects.\n",
    "\n",
    "The goal is to load many posts from a news group, that are about different topics, and to discover clusters of documents that are similar. We will get the topics, but will not use them in the algorithm, because this method is unsupervised and we will see how \"magic\" such an approach is.\n",
    "\n",
    "#### Data:\n",
    "\n",
    "This time, the dataset is a toy dataset that is **available in the scikit library**. Hence, you do not need to download anything yourself. You do, however, require an **Internet connection** in order to let sklearn download it for you.\n",
    "\n",
    "#### Links:\n",
    "\n",
    "- [Clustering Scikit Demo](http://scikit-learn.org/stable/auto_examples/text/document_clustering.html)\n",
    "- [Newsgroup Documentation 1](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#newsgroups)\n",
    "- [Newsgroup Documentation 2](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting by importing our beloved libraries: pandas, numpy, matplotlib.pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load the Data\n",
    "\n",
    "We are using the `fetch_20newsgroups` available in the scikit-learn examples for this notebook. We just need to import the function and call it. We will load a training set and a test set.\n",
    "\n",
    "**Initial Dataset:**\n",
    "\n",
    "The `fetch_20newsgroups` contains a total of 18846 entries and has 20 different categories. Those categories are:\n",
    "\n",
    "- alt.atheism\n",
    "- comp.graphics\n",
    "- comp.os.ms-windows.misc\n",
    "- comp.sys.ibm.pc.hardware\n",
    "- comp.sys.mac.hardware\n",
    "- comp.windows.x\n",
    "- misc.forsale\n",
    "- rec.autos\n",
    "- rec.motorcycles\n",
    "- rec.sport.baseball\n",
    "- rec.sport.hockey\n",
    "- sci.crypt\n",
    "- sci.electronics\n",
    "- sci.med\n",
    "- sci.space\n",
    "- soc.religion.christian\n",
    "- talk.politics.guns\n",
    "- talk.politics.mideast\n",
    "- talk.politics.misc\n",
    "- talk.religion.misc\n",
    "\n",
    "We are going to only select a few of those and, as such, reduce the size of the dataset, because it will be easier to plot the data later. :)\n",
    "\n",
    "**Training dataset:**\n",
    "\n",
    "For the training data, `fetch_20newsgroups` allows us to request it directly and it allows us to remove unneeded parts of the posts. Here, we will remove the headers, footers and quotes.\n",
    "\n",
    "**Test dataset:**\n",
    "\n",
    "For the test data, `fetch_20newsgroups` also allows us to request it directyl. However, here we will leave all parts in. We will see later how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the sklearn example function\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Define our subset of categories\n",
    "categories = ['sci.space', 'comp.windows.x', 'rec.sport.baseball', 'soc.religion.christian']\n",
    "\n",
    "# Request the training data\n",
    "newsgroup_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "\n",
    "# Request the test data\n",
    "newsgroup_test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explore a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looks like a dictionary, so let's print the keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train: Print the size of different attributes\n",
    "print('- Train:')\n",
    "print('{:10} documents'.format( ... ))\n",
    "print('{:10} targets'.format( ... )) # Should be same as #documents\n",
    "print('{:10} categories'.format( ... ))\n",
    "\n",
    "# Test: Print the size of different attributes\n",
    "print('\\n- Test:')\n",
    "print('{:10} documents'.format( ... ))\n",
    "print('{:10} targets'.format( ... )) # Should be same as #documents\n",
    "print('{:10} categories'.format( ... ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show categories (This should be the same as we defined)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Slice / Split the data into training and validation\n",
    "\n",
    "Since, in this example we had a helper function that already gave to us the training and test data, we do not need to split it again. We are ready for the next steps.\n",
    "\n",
    "As reminder, if you had to split, you could use the `train_test_split` function like the following example:\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    train, test = train_test_split(dataset, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Pre-Processing Text Data - Feature Extraction\n",
    "\n",
    "Let's start with pre-processing the data. We are going to focus on 2 aspects:\n",
    "\n",
    "1. Convert the data into a DataFrame. The current data is in a dictionary where the value are lists. We prefer DataFrame, because it is easier to work with big data.\n",
    "2. Process the text and extract features from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The target are the labelled clusters of the dataset\n",
    "# We are lucky to have them, but won't use them much.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First save the targets for later\n",
    "\n",
    "\n",
    "# Just convert the train data to a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First 10 posts of training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First 10 posts of testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the first message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's make use of the random module to just browse through some of the rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is cleaner.\n",
    "\n",
    "**There are a few things to note:**\n",
    "\n",
    "- The training data has been imported via a function that removed headers, footers and quotes from the original message. We chose to not spend too much time on advanded techniques to clean text data, because it is better to focus on the core, which is tokenizing and extracting features.\n",
    "- The test data is original and has not been pre-processed or cleaned.\n",
    "\n",
    "## 6.2 Feature Extraction using TF-IDF\n",
    "\n",
    "To train any kind of machine learning model using text data, we need to, somehow, transform the text into another format that allows an algorithm to do computations.\n",
    "\n",
    "One way to extract features would be to take all words and then count them. Counting is really basic, and not really good for machine learning, because words like 'a', 'an', 'the', 'for', ... appear naturally a lot in an english text.\n",
    "\n",
    "This is why we remove stopwords from the list of extracted words. \n",
    "\n",
    "Still, this is not making things good, because there are words that are not important, or less meaningful, but appear a lot, and words that are very important, or very meaningful, but unfortunatly appear only a few times. \n",
    "\n",
    "Using a count technique, we basically make the most frequent word win!\n",
    "\n",
    "**TF-IDF:**\n",
    "\n",
    "TF-IDF stands for *term frequency inverse document frequency*.\n",
    "\n",
    "I will not go too deep into the details. Check the links below to get more explanations and details about the method.\n",
    "The basic things to know is:\n",
    "\n",
    "- You take the frequency of each words in each document (term frequency)\n",
    "- You take, for each word, the number of documents containing that word (document frequency)\n",
    "\n",
    "The document frequency is high for words that appear a lot (common/boring words) and low for words that are rare.\n",
    "So, dividing the term frequency by the document frequency we get a low score for boring words and a high score for rare words.\n",
    "\n",
    "**Links:**\n",
    "\n",
    "- [TF-IDF Simple Explanation](http://planspace.org/20150524-tfidf_is_about_what_matters/)\n",
    "\n",
    "- [sklearn documentation of TF-IDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "- [TF-IDF Explanation 1](http://www.tfidf.com)\n",
    "- [TF-IDF Explanation Wikipedia](https://en.wikipedia.org/wiki/Tfâ€“idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import TF-IDF from sklearn\n",
    "...\n",
    "\n",
    "# We can give stopwords as parameters already\n",
    "# We can try later what happens if we don't remove stopwords\n",
    "stopwords_english = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
    "\n",
    "# Initialize the TF-IDF object.\n",
    "...\n",
    "\n",
    "# Extract features from the text data\n",
    "...\n",
    "\n",
    "# Print the shape\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Creating, Training & Measuring the Model\n",
    "\n",
    "The next step is to create and train our model and see how we can create our first clusters.\n",
    "\n",
    "The algorithm we are going to is called **K-Means** and it is an unsupervised clustering algorithm. \n",
    "It can take as an input the TF-IDF matrix and also needs an **initial number of clusters**.\n",
    "\n",
    "To understand a bit how the algorithm works, those are the basic steps that it performs:\n",
    "\n",
    "1. Randomly choose k items and define them as initial centroids (= center of cluster). \n",
    "2. For each point, find the nearest centroid and assign the point to the cluster associated with the nearest centroid. \n",
    "3. Update the centroid of each cluster based on the items in that cluster. Typically, the new centroid will be the average of all points in the cluster. \n",
    "4. Repeats steps 2 and 3, till no point switches clusters.\n",
    "\n",
    "**Performance:**\n",
    "\n",
    "\n",
    "\n",
    "**Links:**\n",
    "\n",
    "- [KMeans Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n",
    "- [KMeans Explanation 1](http://bigdata-madesimple.com/possibly-the-simplest-way-to-explain-k-means-algorithm/)\n",
    "- [KMeans Explanation 2 - with animation](https://codeahoy.com/2017/02/19/cluster-analysis-using-k-means-explained/)\n",
    "- [KMeans Explanation 3 - YouTube](https://www.youtube.com/watch?v=RD0nNK51Fp8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Create and Train the Model\n",
    "\n",
    "*The next cell might take a few minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def create_cluster(data, n_clusters, max_iter=300, n_init=10):\n",
    "    print('Creating KMeans Model:')\n",
    "    print('  #Clusters      : {:5}'.format(n_clusters))\n",
    "    print('  Max Iterations : {:5}'.format(max_iter))\n",
    "    print('  #Initalisatons : {:5}'.format(n_init))\n",
    "    \n",
    "    print('\\nIn Progress...\\n')\n",
    "    \n",
    "    # Create the model (Other option: init='k-means++')\n",
    "    ...\n",
    "\n",
    "    # Train the model (\"%time\" computes the execution time in Jupyter notebooks)\n",
    "    ...\n",
    "\n",
    "    print('\\nDone!')\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the number of clusters\n",
    "# We fetched 4 categories, so we are going to use 4 clusters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the computed clusters for each document / row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Check the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can check the clusters for the first 10 documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add it to the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can count the values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_clusters(model, tfidf, n_clusters):\n",
    "    # Get the cluster centers\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    \n",
    "    # Get all the terms\n",
    "    ...\n",
    "\n",
    "    # For each cluster, print the top 50 words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_cluster(matrix, model):\n",
    "    ''' Plot the cluster in 2D. \n",
    "    '''\n",
    "    X = matrix.todense()\n",
    "\n",
    "    pca = PCA(n_components=2).fit(X)\n",
    "    data2D = pca.transform(X)\n",
    "    \n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.scatter(data2D[:,0], \n",
    "                data2D[:,1], \n",
    "                c=model.labels_, \n",
    "                label=\"Documents\")\n",
    "    plt.legend()\n",
    "\n",
    "    centers2D = pca.transform(model.cluster_centers_)\n",
    "\n",
    "    plt.hold(True)\n",
    "    plt.scatter(centers2D[:,0], \n",
    "                centers2D[:,1], \n",
    "                marker='x', \n",
    "                s=200, \n",
    "                linewidths=3, \n",
    "                c='r',\n",
    "                label=\"Cluster Centers\")\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Model Evaluation\n",
    "\n",
    "Let's finish by evaluating our model. We know we need to find some quality metrics in order to have a basis to improve.\n",
    "\n",
    "The challenge here is that evaluating clustering results is much harder than computing the accuracy of a supervised algorithm.\n",
    "\n",
    "So, for this lesson, we are not going to go into the details of this part, because it is more complex than what we plan to cover here. However, the scikit-learn website contains many explanation for those who are most interested in. Check the links below:\n",
    "\n",
    "**Links:**\n",
    "\n",
    "- [Documentation - Clustering Evaluation](http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation)\n",
    "- [Documentation - Clustering Metrics](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "labels = train_target\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, model.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, model.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, model.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, model.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(tfidf_matrix, model.labels_, sample_size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract features from the text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroup_test['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroup_test['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroup_test['text'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trying different models and different data\n",
    "\n",
    "Overview of categories:\n",
    "\n",
    "- alt.atheism\n",
    "- comp.graphics\n",
    "- comp.os.ms-windows.misc\n",
    "- comp.sys.ibm.pc.hardware\n",
    "- comp.sys.mac.hardware\n",
    "- comp.windows.x\n",
    "- misc.forsale\n",
    "- rec.autos\n",
    "- rec.motorcycles\n",
    "- rec.sport.baseball\n",
    "- rec.sport.hockey\n",
    "- sci.crypt\n",
    "- sci.electronics\n",
    "- sci.med\n",
    "- sci.space\n",
    "- soc.religion.christian\n",
    "- talk.politics.guns\n",
    "- talk.politics.mideast\n",
    "- talk.politics.misc\n",
    "- talk.religion.misc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = ['sci.space', 'comp.windows.x', 'rec.sport.baseball', 'soc.religion.christian']\n",
    "\n",
    "# Request the training data\n",
    "data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF object.\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                             min_df=2, \n",
    "                             max_features=10000, \n",
    "                             stop_words=stopwords_english) \n",
    "#                             tokenizer=tokenize_and_stem)\n",
    "\n",
    "# Extract features from the text data\n",
    "tfidf = vectorizer.fit_transform( data['text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the number of clusters\n",
    "# We fetched 4 categories, so we are going to use 4 clusters\n",
    "n_clusters = len(categories)\n",
    "\n",
    "model_2 = create_cluster(tfidf, n_clusters)\n",
    "\n",
    "# Get the computed clusters for each document / row\n",
    "clusters = model_2.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_clusters(model_2, vectorizer, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_cluster(tfidf, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
